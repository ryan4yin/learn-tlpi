# 异步 I/O 模型

> 这一章的笔记相比 TLPI 原文，补充了更现代化的 io_uring 相关内容

异步 I/O 的演进是 Linux 高性能编程的核心脉络。从早期的 **同步多路复用**（如 `select`/`poll`）到
**事件驱动**（`epoll`），再到
**纯异步框架**（`io_uring`），每一代改进都致力于降低延迟、提升吞吐量。本节补充传统 I/O 多路复用模型（`select`/`poll`）的说明，并梳理异步 I/O 的进化历史与优化方向。

## Linux I/O 多路复用模型的演化

### 1. 传统 I/O 多路复用模型：`select` 与 `poll`

#### **1.1 `select`**

- **原理**：通过 `fd_set` 位图监控多个文件描述符，调用 `select()` 阻塞直到至少一个描述符就绪。
- **核心 API**：
  ```c
  fd_set read_fds;
  FD_ZERO(&read_fds);
  FD_SET(fd, &read_fds);
  int ret = select(fd + 1, &read_fds, NULL, NULL, NULL);
  ```
- **缺点**：
  - **线性扫描**：每次调用需遍历所有描述符，时间复杂度为 **O(n)**。
  - **描述符数量限制**：`fd_set` 大小受 `FD_SETSIZE`（通常 1024）限制。
  - **内存重复拷贝**：每次调用需重新传入 `fd_set`。

#### **1.2 `poll`**

- **原理**：通过 `pollfd` 结构体数组管理描述符，支持更多文件描述符，以及一些细节优化。
- **核心 API**：
  ```c
  struct pollfd fds[] = { {fd, POLLIN, 0} };
  int ret = poll(fds, 1, -1);
  ```
- **改进点**：
  - 无固定数量限制（基于动态数组）。
- **遗留问题**：
  - 仍为 **O(n)** 时间复杂度，高并发下性能差。

#### **2.3 信号驱动 I/O（SIGIO）**

- **原理**：通过信号 `SIGIO` 通知进程文件描述符的 I/O 就绪事件。
- **缺点**：
  - 信号处理函数需避免阻塞操作，编程复杂度高。
  - 信号可能丢失或合并，不适合高并发场景。
- **适用场景**：简单设备监控（如串口通信），实际应用较少。

#### **2.4 POSIX AIO（传统异步 I/O）**

- **原理**：通过 `aio_read()`/`aio_write()` 提交异步请求，通过信号或回调函数通知完成。
- **缺点**：
  - 仅支持文件 I/O，网络 I/O 支持不完善。
  - **内核实现效率低，已被 `io_uring` 取代**。

#### **`select`/`poll` 的局限性**

- 每次调用需全量传递描述符集合，内核和用户空间频繁拷贝数据。
- 高并发场景（如 10K 连接）下，CPU 时间浪费在无效遍历上，吞吐量急剧下降。

---

### 2. 现代异步 I/O 模型

##### **2.1 `epoll`（事件驱动 I/O 复用）**

> https://man7.org/linux/man-pages/man7/epoll.7.html

- **定位**：**同步非阻塞多路复用**，通过事件通知机制优化高并发 I/O。
- **核心 API**：
  - `epoll_create()` & `epoll_create1()`：创建 `epoll` 实例。
  - `epoll_ctl()`：注册/修改/删除监控的文件描述符。
  - `epoll_wait()`：等待事件就绪。
- **工作模式**：
  - **水平触发（LT）**：事件未处理时会持续通知（默认模式，编程友好）。
  - **边缘触发（ET）**：仅在状态变化时通知一次（需一次处理所有就绪数据）。
- **优点**：
  - 时间复杂度 O(1)，支持百万级并发连接。
  - 减少无效遍历，性能远超 `select`/`poll`。
- **代码示例**：

  ```c
  int epfd = epoll_create1(0);
  struct epoll_event ev = { .events = EPOLLIN, .data.fd = sockfd };
  epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, &ev);

  while (1) {
    int nready = epoll_wait(epfd, events, MAX_EVENTS, -1);
    for (int i = 0; i < nready; i++) {
      if (events[i].events & EPOLLIN) {
        handle_read(events[i].data.fd);
      }
    }
  }
  ```

#### **水平触发（LT）与边缘触发（ET）的对比与说明**

##### **1. 核心区别**

| **特性**               | **水平触发（LT）**                         | **边缘触发（ET）**                           |
| ---------------------- | ------------------------------------------ | -------------------------------------------- |
| **通知机制**           | 只要文件描述符处于就绪状态，就会持续通知。 | 仅在文件描述符状态发生变化时通知一次。       |
| **数据未处理时的行为** | 内核会重复通知，直到数据被完全读取或写入。 | 内核仅通知一次，后续需用户主动处理剩余数据。 |
| **编程复杂度**         | 较低，适合初学者或简单场景。               | 较高，需结合非阻塞 I/O 避免数据饥饿。        |
| **性能**               | 可能因重复通知引入额外开销。               | 减少无效通知，适合高吞吐场景。               |
| **默认模式**           | `epoll` 的默认模式。                       | 需显式设置 `EPOLLET` 标志启用。              |

##### **2. 适用场景**

- **水平触发（LT）**：
  - **简单应用**：如传统的阻塞式 I/O 改造为多路复用。
  - **调试阶段**：便于观察事件触发逻辑。
  - **长连接服务**：如聊天服务器，需持续监控连接状态。
- **边缘触发（ET）**：
  - **高性能服务器**：如 HTTP 反向代理（Nginx）、高频交易系统。
  - **低延迟需求**：减少内核通知次数，提升吞吐量。
  - **非阻塞 I/O 配合**：必须使用 `O_NONBLOCK` 标志，避免因未读完数据导致事件丢失。

#### **3. 编程注意事项**

##### **水平触发（LT）**

- **优点**：无需担心事件丢失，适合逻辑简单的场景。
- **缺点**：可能因重复通知导致无效唤醒（如数据未及时处理）。
- **示例代码**：
  ```c
  struct epoll_event ev = { .events = EPOLLIN }; // 默认 LT
  epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &ev);
  ```

##### **边缘触发（ET）**

- **必须使用非阻塞 I/O**：避免因 `read()`/`write()` 阻塞导致其他描述符饥饿。
- **需一次性处理所有数据**：循环读取直到 `EAGAIN`/`EWOULDBLOCK`。
- **示例代码**：

  ```c
  struct epoll_event ev = { .events = EPOLLIN | EPOLLET }; // 启用 ET
  fcntl(fd, F_SETFL, O_NONBLOCK); // 非阻塞模式
  epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &ev);

  // 事件处理循环
  while ((n = read(fd, buf, BUF_SIZE)) > 0) {
      process_data(buf, n);
  }
  if (n == -1 && errno != EAGAIN) {
      handle_error();
  }
  ```

#### **4. 常见问题与解决方案**

- **ET 模式下的数据饥饿**：
  - **问题**：未及时处理数据可能导致后续事件丢失。
  - **解决**：确保每次事件触发时完全读取或写入数据。
- **LT 模式的性能瓶颈**：
  - **问题**：高并发下重复通知可能增加 CPU 负载。
  - **解决**：改用 ET 模式或优化事件处理逻辑。

#### **5. 性能对比**

- **吞吐量**：ET 模式在高并发下通常优于 LT，因减少无效通知。
- **延迟**：ET 模式更稳定，适合低延迟场景。
- **调试难度**：LT 更易调试，ET 需额外日志记录事件触发点。

---

##### **2.2 `io_uring`（异步 I/O 的未来）**

> https://kernel.dk/io_uring.pdf

> https://man7.org/linux/man-pages/man7/io_uring.7.html

- **定位**：**纯异步 I/O 框架**，通过共享环形队列实现零拷贝、零系统调用（理想情况下）。
- **核心组件**：
  - **提交队列（SQ）**：用户提交 I/O 请求。
  - **完成队列（CQ）**：内核返回操作结果。
- **核心 API**：
  - `io_uring_setup()`：初始化队列。
  - `io_uring_register()`：注册文件或缓冲区（减少内存拷贝）。
  - `io_uring_submit()`：提交请求。
  - `io_uring_wait_cqe()`：等待完成事件。
- **优势**：
  - **零拷贝**：通过固定缓冲区或 `IORING_OP_READ_FIXED` 避免数据复制。
  - **批量提交**：单次系统调用处理多个 I/O 请求。
  - **全异步**：支持网络、文件、管道等多种 I/O 类型。
- **代码示例**：

  ```c
  struct io_uring ring;
  io_uring_queue_init(32, &ring, 0); // 初始化队列

  // 提交异步读取请求
  struct io_uring_sqe *sqe = io_uring_get_sqe(&ring);
  io_uring_prep_read(sqe, fd, buf, size, 0);
  io_uring_submit(&ring);

  // 等待完成事件
  struct io_uring_cqe *cqe;
  io_uring_wait_cqe(&ring, &cqe);
  printf("Read %d bytes\n", cqe->res);
  io_uring_cqe_seen(&ring, cqe);
  ```

---

### 3. Linux 异步 I/O 的进化历史与核心优化

#### **3.1 演进阶段**

1. **同步阻塞 I/O（BIO）**
   - 每个连接独占线程/进程，资源消耗大，无法应对高并发。
2. **同步多路复用（`select`/`poll`）**
   - 单线程监控多个 I/O 描述符，但性能受限于 **O(n)** 遍历。
3. **事件驱动（`epoll`）**
   - 内核通过红黑树管理描述符，仅返回就绪事件，时间复杂度 **O(1)**。
4. **纯异步框架（`io_uring`）**
   - 用户态与内核态共享环形队列，实现零拷贝、批量提交和全异步处理。

#### **3.2 主要优化点**

| **阶段**        | **优化目标**             | **关键技术**                                 |
| --------------- | ------------------------ | -------------------------------------------- |
| `select`/`poll` | 多路复用                 | 单线程监控多个描述符                         |
| `epoll`         | 减少无效遍历             | 事件驱动、红黑树与就绪链表、ET/LT 模式       |
| `io_uring`      | 消除系统调用与上下文切换 | 环形队列、SQ/CQ 异步交互、内核轮询（SQPOLL） |

- **`epoll` 的突破**：
  - **数据结构优化**：红黑树（高效增删改查） + 就绪链表（直接获取有效事件）。
  - **边缘触发（ET）**：减少事件通知次数，需结合非阻塞 I/O 避免饥饿。
- **`io_uring` 的创新**：
  - **用户与内核共享内存**：提交队列（SQ）和完成队列（CQ）通过内存映射实现零拷贝。
  - **批处理与链式请求**：单次系统调用提交多个 I/O 操作，支持请求依赖关系。

---

### 4. 总结：从 `select` 到 `io_uring` 的性能跃迁

1. **性能指标对比**

   - **系统调用次数**：
     - `select`/`poll`：每次调用 O(n) 遍历，万级连接下频繁调用。
     - `epoll`：仅返回就绪事件，但每次读写仍需单独系统调用。
     - `io_uring`：单次提交批量操作，理想情况下零系统调用（SQPOLL 模式）。
   - **吞吐量**：
     - `epoll` 可轻松支持 10 万级并发，`io_uring` 可达百万级。

2. **适用场景再梳理**

   - **传统系统或低并发**：`epoll` 仍是安全选择。
   - **现代高性能应用**：优先采用 `io_uring`，尤其存储、数据库、高频交易系统。

3. **未来方向**
   - **`io_uring` 生态完善**：更多异步操作支持（如异步信号、定时器）。
   - **硬件协同**：结合 SPDK（用户态 NVMe 驱动）或 DPDK（用户态网络栈）进一步消除内核开销。

##
